# -*- coding: utf-8 -*-
"""webcam_label.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CzBJyzCvRodcKIy6hvZzcc-gMI7wk_Tk
"""

import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib.pyplot as plt

nr_of_trainfeatures = 18000
# each affordance class is set to live 20 dim subspace
nr_of_bases = 20
device = torch.device('cuda:0')#'cpu')

img_size = 224
# object names of the dataset are defined
object_labels = ['ball', 'binder', 'bowl', 'calculator', 'camera','cap', 'cell_phone',
                'cereal_box', 'coffee_mug','comb', 'dry_battery', 'flashlight', 'food_bag',
                'food_box', 'food_can', 'food_cup', 'food_jar', 'glue_stick', 'hand_towel',
                'instant_noodles', 'keyboard','kleenex', 'lightbulb', 'marker', 'notebook',
                'pitcher', 'plate', 'pliers', 'rubber_eraser', 'scissors', 'shampoo',
                 'soda_can', 'sponge', 'stapler', 'toothbrush', 'toothpaste', 'water_bottle']

# listing affordance names to make a dictionary that maps affordance label values to names
affordance_names_T = ['Grasp', 'Wrap Grasp', 'Contain', 'Open', 'Tip-push', 'Display',
                    'Roll', 'Dry', 'Liquid contain', 'Pour', 'Grip', 'Absorb',
                    'Cut', 'Staple', 'Illuminate']
affordance_names = ['grasp', 'wrap grasp', 'containment', 'openable', 'tip-push', 'display',
                    'rollable', 'dry', 'liquid_containment', 'pourable', 'grip', 'absorb',
                    'cut', 'stapling', 'illumination']
#temporary threshold values, they will be calculated more precisely later
threshold_val = 0.65
threshold_val2 = 0.9
threshold_val3 = 0.5

model_name = 'regnet_y'
model_name_T = 'RegNetY'

dataset_name = 'RGBDAffordance'


def get_tens(net_name, datas_name, im_size):
    # state vectors obtained from Resnet are loaded from previously saved file
    st_tns = torch.load(r'/content/%s_with_%s_%dnetworkout.pt'%(net_name, datas_name, im_size))
    st_tns = torch.transpose(st_tns, 0, 1)
    # also results of those tensor are loaded. Results contain affordance labels, like 0, 1, 2,
    rslt_tns = torch.load(r'/content/%s_with_%s_%dlabel.pt'%(net_name, datas_name, im_size))

    # image paths are loaded to name list
    # image names also contain names of the object/s in the image
    nm_list = []
    f = open(r'/content/%s_with_%s_%dimagepaths.txt'%(net_name, datas_name, im_size), 'r')
    for line in f:
        nm_list= line.strip().split('#')
    f.close()

    return st_tns, rslt_tns, nm_list


state_tens, res_tens, name_list = get_tens(model_name, dataset_name, img_size)

nr_of_test = len(name_list)-nr_of_trainfeatures
#taking transpose of the state tensor to make a column matrix
# all results are looked up and affordance classes are listed
afford_labellist = res_tens.unique().tolist()
# '0' is removed since all results include it
afford_labellist.remove(0)
# defining affordance label value to name dictionary
afford_dict = dict()
afford_dict_T = dict()
#print(res_tens[3599, :])

for i in range(len(afford_labellist)):
    afford_dict[afford_labellist[i]] = affordance_names[i]
    afford_dict_T[afford_labellist[i]] = affordance_names_T[i]

# some empty dictionaries to be used later
base_list = dict()
base_point_vecs = dict()
non_base_point_vecs = dict()
affine_baselist = dict()
affine_orth_Ulist = dict()
orth_bases = dict()
W_distances = dict()
state_dict = dict()
non_state_dict = dict()
threshold_dict = dict()
threshold_angdict = dict()
ratio_vals = dict()
non_ratio_vals = dict()
nr_of_bases_dict = dict()
subs_angls = dict()

def project_matr(bases):
    return torch.matmul(torch.matmul(bases, torch.linalg.inv(
        torch.matmul(torch.transpose(bases, 0, 1), bases) )), torch.transpose(bases, 0, 1))

def matr_to_origin(tens, indx):
    [dim1, dim2] = tens.shape
    t = tens[:, indx]
    t = t.resize(dim1, 1).expand(dim1, dim2)
    return tens-t

def matr_vec_to_origin(tens, vec):
    [dim1, dim2] = tens.shape
    vec = vec.resize(dim1, 1).expand(dim1, dim2)
    return tens-vec

def matr_zero_mean(M):
    [dim1, dim2] = M.shape
    mn = torch.mean(M, 1)
    mn = mn.resize(dim1, 1).expand(dim1, dim2)
    return M-mn

def calc_mi(m, ind,rdim, nnum):
    mto = matr_to_origin(m, ind)
    dig = torch.diag(torch.matmul(torch.transpose(mto, 0, 1), mto))
    salad, ind = torch.sort(dig)
    ind_e = ind[1:nnum+1]
    mi= matr_zero_mean(m[:, ind_e])
    u, s, vh = torch.linalg.svd(mi, full_matrices=True)
    return( torch.matmul(u[:,:rdim], torch.diag(s[:rdim])), s[:rdim], ind_e)

def curv_calc(m, vct, rdim, nnum):
    mtp = matr_vec_to_origin(m, vct)
    dig = torch.diag(torch.matmul(torch.transpose(mtp, 0, 1), mtp))
    salad, ind = torch.sort(dig)
    ind_e = ind[0:nnum]
    ind_w = ind[1:nnum]
    mn = m[:, ind_e]
    mni = matr_zero_mean(mn)
    #mw = m[:, ind_w]
    #mwi = matr_zero_mean(mw)
    mtot = torch.concat((mn, vct), 1)
    mtoti = matr_zero_mean(mtot)

    un, sn, vn = torch.linalg.svd(mni, full_matrices = True)
    utot, stot, vtot = torch.linalg.svd(mtoti, full_matrices = True)
    #uw, sw, vw = torch.linalg.svd(mwi, full_matrices = True)

    usn = torch.matmul(un[:,:rdim], torch.diag(sn[:rdim]))
    ustot = torch.matmul(utot[:,:rdim], torch.diag(stot[:rdim]))
    #usw = torch.matmul(uw[:,:rdim], torch.diag(sw[:rdim]))

    Q = torch.matmul(torch.transpose(usn, 0, 1), ustot)
    uq, sq, vq = torch.linalg.svd(Q)
    theta = torch.acos(torch.abs(torch.clamp(torch.sum(sq)/torch.sum(sn[:rdim]*stot[:rdim]), min=-1.0, max=1.0)))
    return theta#/theta_w

def get_afford_features(indc):
    afford_sts = state_tens[:, indc.squeeze()]
    mean_vl = torch.mean(afford_sts, 1)
    [dim1, dim2] = afford_sts.shape
    base_ts = mean_vl.resize(dim1, 1).expand(dim1, dim2)
    origin_zero_mtr = afford_sts-base_ts
    U, S, Vh = torch.linalg.svd(origin_zero_mtr, full_matrices = True)
    Uaff, Saff, Vhaff = torch.linalg.svd(afford_sts, full_matrices = True)

    return afford_sts , mean_vl, project_matr( U[:, :nr_of_bases]), project_matr(Uaff[:, :nr_of_bases]), Uaff[:,:nr_of_bases]

red_dim = 3
num_n = 10
angl_list_tot = []
for i in afford_labellist:
    indices = torch.nonzero(torch.sum( (res_tens[:nr_of_trainfeatures, :] == i).int() , axis = 1))
    non_indices = torch.nonzero(torch.sum( (res_tens[:nr_of_trainfeatures, :] == i).int() , axis = 1)==0)
    afford_states = state_tens[:, indices.squeeze()]
    non_afford_states = state_tens[:, non_indices.squeeze()]
    state_dict[i] = afford_states.to(device)
    non_state_dict[i] = non_afford_states

    mean_val = torch.mean(afford_states,1)
    non_mean_val = torch.mean(non_afford_states, 1)
    base_point_vecs[i] = mean_val.to(device)
    non_base_point_vecs[i] = non_mean_val.to(device)
    [dim1, dim2] = afford_states.shape
    base_tens = mean_val.resize(dim1, 1).expand(dim1, dim2)
    origin_zero_matr = afford_states-base_tens
    U, S, Vh = torch.linalg.svd(origin_zero_matr, full_matrices = True)

    nr_of_bases_dict[i] = torch.tensor([20])
    Uaff, Saff, Vhaff = torch.linalg.svd(afford_states, full_matrices = True)
    base_list[i] =project_matr( U[:, :nr_of_bases_dict[i]]).to(device)
    affine_baselist[i] = project_matr(Uaff[:, :nr_of_bases_dict[i]])
    affine_orth_Ulist[i] = Uaff[:,:nr_of_bases_dict[i]]


for i in afford_labellist:
    aff_states = state_dict[i]
    mean_v = base_point_vecs[i]
    [dim1, dim2] = aff_states.shape
    base_origin = mean_v.resize(dim1, 1).expand(dim1, dim2)
    origin_tozero_matr = aff_states.to(device)-base_origin
    projections = torch.matmul(base_list[i], origin_tozero_matr)
    non_aff_states = non_state_dict[i]
    non_mean_v = non_base_point_vecs[i]
    [dim1, dim2] = non_aff_states.shape
    non_base_origin = non_mean_v.resize(dim1, 1).expand(dim1, dim2)
    non_origin_tozero_matr = non_aff_states.to(device)-non_base_origin
    non_projections = torch.matmul(base_list[i], non_origin_tozero_matr)


    ratio_vals[i] = torch.div(torch.norm(projections, dim = 1), torch.norm(origin_tozero_matr, dim = 1))
    non_ratio_vals[i] = torch.div(torch.norm(non_projections, dim = 1), torch.norm(non_origin_tozero_matr, dim = 1))
    base_matr = affine_orth_Ulist[i]



    ref_range = 100

    true_pos_rat = []
    false_pos_rat = []
    ref_list  = []
    opt_list = []


    for k in range(ref_range):
        ref_val = k/ref_range
        ref_list.append(ref_val)
        true_pos = torch.sum(ratio_vals[i]>ref_val).item()
        false_neg = torch.sum(ratio_vals[i]<=ref_val).item()

        true_neg = torch.sum(non_ratio_vals[i]<=ref_val).item()
        false_pos = torch.sum(non_ratio_vals[i]>ref_val).item()

        tpr = true_pos/(true_pos+false_neg)
        true_pos_rat.append(tpr)
        fpr = false_pos/(false_pos+true_neg)
        false_pos_rat.append(fpr)
        opt_list.append((fpr**2)+(1-tpr)**2)
        #print('tp, fn, tn, fp:',true_pos, false_neg, true_neg, false_pos)
    save_thresh = ref_list[opt_list.index(min(opt_list))]
    threshold_dict[i] = save_thresh

    for j in afford_labellist:
        if i != j:
            base_matr = base_matr - torch.matmul(affine_baselist[j], base_matr)

    orth_bases[i] = (torch.norm(affine_orth_Ulist[i])/torch.norm(base_matr)).item()*project_matr(base_matr)


test_indices = 21400



accuracy_list_p = []
accuracy_list_n = []

ang_acc_p = []
ang_acc_n = []