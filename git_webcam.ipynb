{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWK9XgZPAcwHdL2JgnYeBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ismailozcil/affordance_git/blob/main/git_webcam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OTomxRrDpkG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as pil_im\n",
        "%matplotlib inline\n",
        "import  scipy.io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "device = torch.device('cpu')#('cuda:0')\n",
        "\n",
        "class SquarePad:\n",
        "    def __call__(self, image):\n",
        "        s = image.shape\n",
        "        max_wh = max(s[-1], s[-2])\n",
        "        hp = int((max_wh - s[-1]) / 2)\n",
        "        vp = int((max_wh - s[-2]) / 2)\n",
        "        padding = (hp, hp, vp, vp)\n",
        "        return F.pad(image, padding, 'constant', 0)\n",
        "\n",
        "class norm_to_zo:\n",
        "    def __call__(self, image):\n",
        "        return image/255.0\n",
        "\n",
        "\n",
        "def get_net_out(img):\n",
        "    model_conv = models.regnet_y_16gf(pretrained = True)\n",
        "    for param in model_conv.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model_feature = torch.nn.Sequential(*(list(model_conv.children())[:-1]))\n",
        "    model_feature = model_feature.to(device)\n",
        "    model_feature.eval()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    SquarePad(),\n",
        "    #norm_to_zo(),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img = transform(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        img_tens = model_feature(img)\n",
        "    return img_tens\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize the output to fit the video element.\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            // Wait for Capture to be clicked.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "        ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    pil_img = pil_im.open(filename)\n",
        "\n",
        "# each affordance class is set to live 20 dim subspace\n",
        "nr_of_bases = 20\n",
        "img_size = 224\n",
        "# object names of the dataset are defined\n",
        "object_labels = ['ball', 'binder', 'bowl', 'calculator', 'camera','cap', 'cell_phone',\n",
        "                'cereal_box', 'coffee_mug','comb', 'dry_battery', 'flashlight', 'food_bag',\n",
        "                'food_box', 'food_can', 'food_cup', 'food_jar', 'glue_stick', 'hand_towel',\n",
        "                'instant_noodles', 'keyboard','kleenex', 'lightbulb', 'marker', 'notebook',\n",
        "                'pitcher', 'plate', 'pliers', 'rubber_eraser', 'scissors', 'shampoo',\n",
        "                 'soda_can', 'sponge', 'stapler', 'toothbrush', 'toothpaste', 'water_bottle']\n",
        "\n",
        "# listing affordance names to make a dictionary that maps affordance label values to names\n",
        "affordance_names_T = ['Grasp', 'Wrap Grasp', 'Contain', 'Open', 'Tip-push', 'Display',\n",
        "                    'Roll', 'Dry', 'Liquid contain', 'Pour', 'Grip', 'Absorb',\n",
        "                    'Cut', 'Staple', 'Illuminate']\n",
        "affordance_names = ['grasp', 'wrap grasp', 'containment', 'openable', 'tip-push', 'display',\n",
        "                    'rollable', 'dry', 'liquid_containment', 'pourable', 'grip', 'absorb',\n",
        "                    'cut', 'stapling', 'illumination']\n",
        "#temporary threshold values, they will be calculated more precisely later\n",
        "threshold_val = 0.65\n",
        "threshold_val2 = 0.9\n",
        "threshold_val3 = 0.5\n",
        "\n",
        "#model_name = 'regnet_y'\n",
        "#model_name_T = 'RegNetY'\n",
        "model_name = 'resnet18'\n",
        "model_name_T = 'ResNet18'\n",
        "dataset_name = 'RGBDAffordance'\n",
        "\n",
        "\n",
        "def get_tens(net_name, datas_name, im_size):\n",
        "    # state vectors obtained from Resnet are loaded from previously saved file\n",
        "    st_tns = torch.load(r'/content/%s_with_%s_%dnetworkout.pt'%(net_name, datas_name, im_size))\n",
        "    st_tns = torch.transpose(st_tns, 0, 1)\n",
        "    # also results of those tensor are loaded. Results contain affordance labels, like 0, 1, 2,\n",
        "    rslt_tns = torch.load(r'/content/%s_with_%s_%dlabel.pt'%(net_name, datas_name, im_size))\n",
        "\n",
        "    # image paths are loaded to name list\n",
        "    # image names also contain names of the object/s in the image\n",
        "    nm_list = []\n",
        "    f = open(r'/content/%s_with_%s_%dimagepaths.txt'%(net_name, datas_name, im_size), 'r')\n",
        "    for line in f:\n",
        "        nm_list= line.strip().split('#')\n",
        "    f.close()\n",
        "\n",
        "    return st_tns, rslt_tns, nm_list\n",
        "\n",
        "def project_matr(bases):\n",
        "    return torch.matmul(torch.matmul(bases, torch.linalg.inv(\n",
        "        torch.matmul(torch.transpose(bases, 0, 1), bases) )), torch.transpose(bases, 0, 1))\n",
        "\n",
        "def matr_to_origin(tens, indx):\n",
        "    [dim1, dim2] = tens.shape\n",
        "    t = tens[:, indx]\n",
        "    t = t.resize(dim1, 1).expand(dim1, dim2)\n",
        "    return tens-t\n",
        "\n",
        "def matr_vec_to_origin(tens, vec):\n",
        "    [dim1, dim2] = tens.shape\n",
        "    vec = vec.resize(dim1, 1).expand(dim1, dim2)\n",
        "    return tens-vec\n",
        "\n",
        "def matr_zero_mean(M):\n",
        "    [dim1, dim2] = M.shape\n",
        "    mn = torch.mean(M, 1)\n",
        "    mn = mn.resize(dim1, 1).expand(dim1, dim2)\n",
        "    return M-mn\n",
        "\n",
        "def calc_mi(m, ind,rdim, nnum):\n",
        "    mto = matr_to_origin(m, ind)\n",
        "    dig = torch.diag(torch.matmul(torch.transpose(mto, 0, 1), mto))\n",
        "    salad, ind = torch.sort(dig)\n",
        "    ind_e = ind[1:nnum+1]\n",
        "    mi= matr_zero_mean(m[:, ind_e])\n",
        "    u, s, vh = torch.linalg.svd(mi, full_matrices=True)\n",
        "    return( torch.matmul(u[:,:rdim], torch.diag(s[:rdim])), s[:rdim], ind_e)\n",
        "\n",
        "def curv_calc(m, vct, rdim, nnum):\n",
        "    mtp = matr_vec_to_origin(m, vct)\n",
        "    dig = torch.diag(torch.matmul(torch.transpose(mtp, 0, 1), mtp))\n",
        "    salad, ind = torch.sort(dig)\n",
        "    ind_e = ind[0:nnum]\n",
        "    ind_w = ind[1:nnum]\n",
        "    mn = m[:, ind_e]\n",
        "    mni = matr_zero_mean(mn)\n",
        "    #mw = m[:, ind_w]\n",
        "    #mwi = matr_zero_mean(mw)\n",
        "    mtot = torch.concat((mn, vct), 1)\n",
        "    mtoti = matr_zero_mean(mtot)\n",
        "\n",
        "    un, sn, vn = torch.linalg.svd(mni, full_matrices = True)\n",
        "    utot, stot, vtot = torch.linalg.svd(mtoti, full_matrices = True)\n",
        "    #uw, sw, vw = torch.linalg.svd(mwi, full_matrices = True)\n",
        "\n",
        "    usn = torch.matmul(un[:,:rdim], torch.diag(sn[:rdim]))\n",
        "    ustot = torch.matmul(utot[:,:rdim], torch.diag(stot[:rdim]))\n",
        "    #usw = torch.matmul(uw[:,:rdim], torch.diag(sw[:rdim]))\n",
        "\n",
        "    Q = torch.matmul(torch.transpose(usn, 0, 1), ustot)\n",
        "    uq, sq, vq = torch.linalg.svd(Q)\n",
        "    theta = torch.acos(torch.abs(torch.clamp(torch.sum(sq)/torch.sum(sn[:rdim]*stot[:rdim]), min=-1.0, max=1.0)))\n",
        "    return theta#/theta_w\n",
        "\n",
        "def curv_calc_auto(m, vct, thresh_vl):\n",
        "    mtp = matr_vec_to_origin(m, vct)\n",
        "    dig = torch.diag(torch.matmul(torch.transpose(mtp, 0, 1), mtp))\n",
        "    salad, ind = torch.sort(dig)\n",
        "\n",
        "    neighbour_number_list = list()\n",
        "    neighbour_dim_list = list()\n",
        "\n",
        "    for indx_num in range(2,100):\n",
        "        #print(indx_num)\n",
        "        ind_e = ind[0:indx_num]\n",
        "        ind_w = ind[1:indx_num]\n",
        "        mn = m[:, ind_e]\n",
        "        mni = matr_zero_mean(mn)\n",
        "        #mw = m[:, ind_w]\n",
        "        #mwi = matr_zero_mean(mw)\n",
        "        mtot = torch.concat((mn, vct), 1)\n",
        "        mtoti = matr_zero_mean(mtot)\n",
        "\n",
        "        un, sn, vn = torch.linalg.svd(mni, full_matrices = True)\n",
        "        utot, stot, vtot = torch.linalg.svd(mtoti, full_matrices = True)\n",
        "        #uw, sw, vw = torch.linalg.svd(mwi, full_matrices = True)\n",
        "        #print(sn)\n",
        "\n",
        "        energy_tensor = torch.cumsum(sn, dim = 0)/torch.sum(sn, dim = 0)\n",
        "        #print(energy_tensor)\n",
        "        #print(energy_tensor)\n",
        "        try:\n",
        "            rdim = torch.min((energy_tensor>thresh_vl).nonzero().squeeze()).item()\n",
        "        except:\n",
        "            rdim = 0\n",
        "        neighbour_number_list.append(indx_num)\n",
        "        neighbour_dim_list.append(rdim)\n",
        "        if 3<=indx_num:\n",
        "            if rdim<=neighbour_dim_list[-2]:\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    usn = torch.matmul(un[:,:rdim], torch.diag(sn[:rdim]))\n",
        "    ustot = torch.matmul(utot[:,:rdim], torch.diag(stot[:rdim]))\n",
        "    #usw = torch.matmul(uw[:,:rdim], torch.diag(sw[:rdim]))\n",
        "    \"\"\"\n",
        "\n",
        "    Qw = torch.matmul(torch.transpose(usn, 0, 1), usw)\n",
        "    uqw, sqw, vqw = torch.linalg.svd(Qw)\n",
        "    theta_w = torch.acos(torch.abs(torch.clamp(torch.sum(sqw)/torch.sum(sn[:rdim]*sw[:rdim]), min=-1.0, max=1.0)))\n",
        "    \"\"\"\n",
        "\n",
        "    Q = torch.matmul(torch.transpose(usn, 0, 1), ustot)\n",
        "    uq, sq, vq = torch.linalg.svd(Q)\n",
        "    theta = torch.acos(torch.abs(torch.clamp(torch.sum(sq)/torch.sum(sn[:rdim]*stot[:rdim]), min=-1.0, max=1.0)))\n",
        "    return theta#/theta_w\n",
        "\n",
        "def optimal_thresh(prjct_matr, origin_zero, non_origin_zero):\n",
        "    prjcts = torch.matmul(prjct_matr, origin_zero)\n",
        "    non_prjcts = torch.matmul(prjct_matr, non_origin_zero)\n",
        "    ratio_vls = torch.div(torch.norm(prjcts, dim = 0), torch.norm(origin_zero, dim = 0))\n",
        "    non_ratio_vls = torch.div(torch.norm(non_prjcts, dim = 0), torch.norm(non_origin_zero, dim = 0))\n",
        "\n",
        "    ref_range = 100\n",
        "    true_pos_rat = []\n",
        "    false_pos_rat = []\n",
        "    ref_list = []\n",
        "    opt_list = []\n",
        "\n",
        "    for k in range(ref_range):\n",
        "        ref_val = k/ref_range\n",
        "        ref_list.append(ref_val)\n",
        "        true_pos = torch.sum(ratio_vls>ref_val).item()\n",
        "        false_neg = torch.sum(ratio_vls<=ref_val).item()\n",
        "\n",
        "        true_neg = torch.sum(non_ratio_vls<=ref_val).item()\n",
        "        false_pos = torch.sum(non_ratio_vls>ref_val).item()\n",
        "\n",
        "        tpr = true_pos/(true_pos+false_neg)\n",
        "        true_pos_rat.append(tpr)\n",
        "        fpr = false_pos/(false_pos+true_neg)\n",
        "        false_pos_rat.append(fpr)\n",
        "        opt_list.append((fpr**2)+(1-tpr)**2)\n",
        "        #print('tp, fn, tn, fp:',true_pos, false_neg, true_neg, false_pos)\n",
        "    save_thresh = ref_list[opt_list.index(min(opt_list))]\n",
        "    return(ratio_vls, non_ratio_vls, ref_list, opt_list, save_thresh, true_pos_rat, false_pos_rat)\n",
        "\n",
        "def get_subspace_bases():\n",
        "    state_tens, res_tens, name_list = get_tens(model_name, dataset_name, img_size)\n",
        "    #taking transpose of the state tensor to make a column matrix\n",
        "    # all results are looked up and affordance classes are listed\n",
        "    afford_labellist = res_tens.unique().tolist()\n",
        "    # '0' is removed since all results include it\n",
        "    afford_labellist.remove(0)\n",
        "    # defining affordance label value to name dictionary\n",
        "    afford_dict = dict()\n",
        "    afford_dict_T = dict()\n",
        "\n",
        "    for i in range(len(afford_labellist)):\n",
        "        afford_dict[afford_labellist[i]] = affordance_names[i]\n",
        "        afford_dict_T[afford_labellist[i]] = affordance_names_T[i]\n",
        "\n",
        "    # some empty dictionaries to be used later\n",
        "    base_list = dict()\n",
        "    base_point_vecs = dict()\n",
        "    non_base_point_vecs = dict()\n",
        "    state_dict = dict()\n",
        "    non_state_dict = dict()\n",
        "    threshold_dict = dict()\n",
        "    ratio_vals = dict()\n",
        "    non_ratio_vals = dict()\n",
        "    nr_of_bases_dict = dict()\n",
        "    subs_angls = dict()\n",
        "\n",
        "\n",
        "    red_dim = 3\n",
        "    num_n = 10\n",
        "    angl_list_tot = []\n",
        "    plt.figure()\n",
        "    for i in afford_labellist:\n",
        "        print(i)\n",
        "        indices = torch.nonzero(torch.sum( (res_tens == i).int() , axis = 1))\n",
        "        non_indices = torch.nonzero(torch.sum( (res_tens == i).int() , axis = 1)==0)\n",
        "        afford_states = state_tens[:, indices.squeeze()]\n",
        "        non_afford_states = state_tens[:, non_indices.squeeze()]\n",
        "        state_dict[i] = afford_states.to(device)\n",
        "\n",
        "        mean_val = torch.mean(afford_states,1)\n",
        "        non_mean_val = torch.mean(non_afford_states, 1).to(device)\n",
        "        base_point_vecs[i] = mean_val.to(device)\n",
        "        [dim1, dim2] = afford_states.shape\n",
        "        base_tens = mean_val.resize(dim1, 1).expand(dim1, dim2)\n",
        "        origin_zero_matr = afford_states-base_tens\n",
        "        U, S, Vh = torch.linalg.svd(origin_zero_matr, full_matrices = True)\n",
        "        ratio_tens = torch.zeros(afford_states.shape)\n",
        "        non_ratio_tens = torch.zeros(non_afford_states.shape)\n",
        "        all_ratio_tens = torch.zeros(torch.cat((ratio_tens, non_ratio_tens), 1).shape)\n",
        "\n",
        "        [dim1, dim2] = non_afford_states.shape\n",
        "        non_base_origin = non_mean_val.resize(dim1, 1).expand(dim1, dim2)\n",
        "        non_origin_tozero_matr = non_afford_states.to(device)-non_base_origin\n",
        "\n",
        "\n",
        "        ratio_tens = torch.abs(torch.div(torch.matmul(torch.transpose(U,0,1), origin_zero_matr), torch.norm(origin_zero_matr, dim = 0).unsqueeze(0).expand(origin_zero_matr.shape[0],-1)))\n",
        "        non_ratio_tens = torch.abs(torch.div(torch.matmul(torch.transpose(U,0,1), non_origin_tozero_matr), torch.norm(non_origin_tozero_matr, dim = 0).unsqueeze(0).expand(non_origin_tozero_matr.shape[0],-1)))\n",
        "        state_proj_mean = torch.mean(ratio_tens, 1)\n",
        "        non_state_proj_mean = torch.mean(non_ratio_tens, 1)\n",
        "        indc = torch.nonzero(state_proj_mean>non_state_proj_mean).squeeze().int()\n",
        "        rtls, non_ratls, ref_list, opt_list, save_thresh, true_pos_rat, false_pos_rat = optimal_thresh(project_matr( U[:,indc]).to(device), origin_zero_matr, non_origin_tozero_matr)\n",
        "        threshold_dict[i] = save_thresh\n",
        "        base_list[i] =project_matr( U[:, indc]).to(device)\n",
        "        plt.plot(false_pos_rat, true_pos_rat, linewidth=3.0, label = afford_dict_T[i])\n",
        "        plt.scatter(false_pos_rat[opt_list.index(min(opt_list))], true_pos_rat[opt_list.index(min(opt_list))], color = 'r', zorder =1000, s = 60)\n",
        "\n",
        "\n",
        "    plt.xlabel('False Positive Ratio')\n",
        "    plt.ylabel('True Positive Ratio')\n",
        "    plt.grid()\n",
        "    #plt.legend()\n",
        "    plt.title('ROC Curves of Affordance Groups for %s'%model_name_T)\n",
        "    plt.savefig('ROC_Curve_%s.png'%model_name_T.replace(\"/\", \"\"))\n",
        "    return state_dict, afford_labellist, afford_dict, afford_dict_T, base_list, base_point_vecs, threshold_dict, ratio_tens, non_ratio_tens"
      ]
    }
  ]
}